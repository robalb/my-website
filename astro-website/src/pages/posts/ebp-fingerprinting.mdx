---
layout: '../../layouts/BlogPost.astro'
title: Implementing fast TCP fingerprinting with eBPF
subtitle: 
publishDate: 2025-06-01
description: 
tags: ['ebpf', 'tcp', 'fingerprinting']
permalink: https://halb.it/posts/ebpf-fingerprinting/
---
import Picture from '../../components/Picture.astro'
import Spoiler from '../../components/Spoiler.astro'

In this article I want to document my journey implementing fast TCP fingerprinting 
in a golang webserver, using eBPF.  

TCP fingerprinting is just one of the many signals that can be used to detect
unusual or identifying informations about a web request in the context of anti-bot solutions.  
Implementing such a system offers interesting technical challenges that span different domains, 
which is why I believe it's an experience worth sharing.

I split this article in two parts.
- In the [first part](#), which you are reading right now, I provide a background on TCP fingerprinting, and discuss some implementation strategies.
- In the [second part](#2) I describe the actual development and testing of an XDP eBPF program capable
of handling 15.000 requests per second.
As a proof-of-concept, I implemented a standalone webserver that echoes back
a dump of the TCP SYN of the client.

### HTTP requests, from first principles

(In order to fully understand what we want to do, I believe it's useful to start from what we know about web servers, and analize our problem from first principles.)

In order to fully understand what want to do, I believe it's useful to start analyzing the
problem from first principles, which in this case means understandig web servers.

Luckily for us, at its essence an HTTP webserver is extremely simple.  
This should't be surprising: although browsers and the underlying protocols evolved and got [more complex](https://en.wikipedia.org/wiki/HTTP/3) over time, for 
compatibility reasons they 
still support the HTTP/1.0 protocol, which was [designed to be simple](https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/Evolution_of_HTTP).
An HTTP/1.0 web server is just a light layer on top of a TCP connection, and can be implemented in just a few lines of C code.


As an example, I implemented a simple hello world webserver in C that we'll use as 
a starting point for the experiments in this article.
You don't really have to read the code for now, but keep in mind that it's here:

<Spoiler text="See code">

```c
#include <netinet/in.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>

#define PORT 8080

int main() {
  // Create socket file descriptor
  int server_fd = socket(AF_INET, SOCK_STREAM, 0);
  if (server_fd == 0) {
    perror("socket failed");
    exit(EXIT_FAILURE);
  }

  // Create a configuration struct with the network address and port
  struct sockaddr_in address = {
        .sin_family = AF_INET,
        .sin_addr.s_addr = INADDR_ANY,
        .sin_port = htons(PORT)
    };
  socklen_t address_length = sizeof(address);
  struct sockaddr *address_pointer = (struct sockaddr *)&address;

  // Bind the socket to the network address and port
  if (bind(server_fd, address_pointer, address_length) < 0) {
    perror("bind failed");
    exit(EXIT_FAILURE);
  }

  // Listen for connections
  if (listen(server_fd, 1) < 0) {
    perror("listen failed");
    exit(EXIT_FAILURE);
  }

  printf("Server listening on port %d...\n", PORT);

  while (1) {
    // Accept a new connection
    int new_socket = accept(server_fd, address_pointer, &address_length);
    if (new_socket < 0) {
      perror("accept failed");
      exit(EXIT_FAILURE);
    }

    // Send a response
    const char *response = "HTTP/1.0 200 OK\r\n"
                           "Content-Type: text/plain\r\n"
                           "Content-Length: 12\r\n"
                           "\r\n"
                           "Hello world ";

    send(new_socket, response, strlen(response), 0);

    // Close the connection
    close(new_socket);
  }

  return 0;
}
```

</Spoiler>

The code is not even remotely close to implement a real webserver,
but it serves the purpose of showing the inner working of an HTTP/1 connection.
If we compile the code, run it, and visit `http://127.0.0.1:8080`
from any modern browser, we'll be greeted by a hello world.

<Picture src="ebpf-fingerprint-browser1" height={490} alt="" />
<br/>

Beautiful, isn't it?

This code differs significantly from the hello worlds that you would normally write
with a modern web framework, regardles of what's trending today, for an important reason:  
We are directly using the abstraction layer provided by the operative system,
Instead of using the abstractions that web frameworks provide to simplify the creation
of a web server.

All the code in this example can be broken down into a few calls of the standard library.
Those functions are just thin wrappers around [syscalls](https://syscalls.mebeim.net/?table=x86/64/x64/latest) whith a similar name, which are all we need to implement a webserver on a POSIX system:

- we create a `socket`
- we `bind` it to a ip and port, and start to `listen`
- in an infinite loop, we call `accept()`, which blocks the execution
  until a client connects to our server
- after a connection is made, accept returns a file descriptor
  which we can `read` and `write` to like a simple file. At the other end of the
  line, the client will communicate with us in a similar way.

It's easy to see that beyond defining an ip and port we don't 
have to write any networking-related code, the OS is taking care of that for us.  
Because of that, if we want to understand what's happening at a deeper level we need to
capture and inspect the network traffic that reaches our webserver, which is something that
we can easily do with wireshark.

<Picture src="ebpf-fingerprint-wireshark1_crop" alt="" />
<br/>

To capture the packets shown in this screenshot I launched wireshark on the loopback interface, then I visited
the hello world website from the browser in order to generate some data.

With this webserver running and with our example traffic capture at hand, we can now
start to experiment with tcp traffic.

### Anatomy of a TCP handshake

Without going too much into details, TCP connections start with a famous three-way handhshake,
During which both the client and the server exchange some informations that 
are useful to establish a reliable connection.  

```
Client                      Server
  |                           |
  | --------- SYN ----------> |  Step 1: Client sends SYN
  |                           |
  | <----- SYN + ACK -------- |  Step 2: Server responds with SYN + ACK
  |                           |
  | --------- ACK ----------> |  Step 3: Client sends ACK
  |                           |
```

Historically, the informations exchanged during the handshake have always provided useful insight into 
the identity of the client. 

Let's have a look at the first TCP SYN in my packet captures:

<Picture src="ebpf-fingerprint-syn-color" alt="" />
<br/>

Broadly speaking, there are two kinds of informations we can gather from this data:
- informations about the Device that generated the packet
- informations about the route the packet travelled trough

A good overview of all these informations is provided by the authors
of the [ja4t TCP fingerprint](https://medium.com/foxio/ja4t-tcp-fingerprinting-12fb7ce9cb5a),
in an article I highly recommend. 
It's not my intention to repeat what's already been written extensively on the topic,
but in order to provide at least one example on the kind of data we gan gather from a packet,
i'll talk about one TCP option: MSS.

the Maximum Segment Size (MSS) TCP option can provide useful informations on the route of the packet.
In the packet we are analyzing, it's the value
highlighted in purple.  
This value represents the maximum size of TCP payload data that can be sent per packet,
and is dependent on the overhead in the network connection.  
For example, over ethernet we usually cannot send more than 1500 bytes over a single transaction,
so the MSS will be 1500 minus the size of the TCP and IP headers.

<Picture src="ebpf-fingerprint-tcp2z" height={100} alt="" />
<br/>

Observing an mss of 1460 would indicate a standard connection over ethernet.
a value smaller than that could indicate a vpn service. Or just a mobile network connection,
or GRE, or MSS clamping caused by [middleboxes](https://tma.roc.cnam.fr/Proceedings/TMA_Paper_22.pdf)...  
Network connections can be complicated, and we can't expect to learn 
everything from a single value. But at least it can provide some insight,
and some entropy for a fingerprinting system.

<Picture src="ebpf-fingerprint-tcp3z2" height={110} alt="" />
<br/>

Now, What about the MSS in the packet we are analyzing? 
If you look carefully, you'll see that the MSS is `65495`, 
way higher than what can fit in an Ethernet frame, and way off of 
any value we talked about so far.  
In a way, this is telling us something about the route too: 
`64Kb` is the [MTU of the loopback interface](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=0cf833aefaa85bbfce3ff70485e5534e09254773) on Linux.
This data is telling us that the packet never left the kernel, 
and lived in captivity without ever travelling the world.

### TCP fingerprinting: Making a plan

Everything I talked about so far was just a big detour from the actual goal of this project,
which is to write a webserver that will echo back to the visitor 
its TCP fingerprinting data.  
For something like this to work, the webserver needs to directly 
access the TCP SYN data of the connections it receives.  
Is this something that we can do with the existing APIs we are using?

As it turns out, not really.
When our server accepts a new client connection, the `accept()` function returns
a new file descriptor.
We can use [getsockopt](https://man.archlinux.org/man/getsockopt.2.en) on that file descriptor
to receive some useful informations on the ongoing connection:
```c
getsockopt(sockfd, IPPROTO_TCP, TCP_INFO, &info, &len)
```

`IPPROTO_TCP` indicates that we are interested on the tcp-layer of the socket connection
`TCP_INFO` asks to receive information on the connection, causing the function to return a 
[tcp_info struct](https://github.com/torvalds/linux/blob/5b032cac622533631b8f9b7826498b7ce75001c6/include/uapi/linux/tcp.h#L229).

Sadly, if you have a quick look at the contents of that struct, you'll see that it only contains 
informations on the ongoing connection, nothing related to the original TCP SYN data.
There is a `tcpi_rcv_mss` value for example, but that's the result of a negotiation, 
not what the client originally sent.  
That's all we can get from the POSIX APIs.

### LibPCAP

The POSIX APIs don't provide easy access to TCP SYNs, but we can follow the same 
approach as wireshark, and just capture raw packets in a userspace buffer.
We can do this very easily with the LibPCAP library, which offers out of the box
a very simple system for filtering and capturing network packets.

A simple approach to implement our webserver could work like this:

- First, we start LibPCAP in a separate thread, capturing TCP SYN packets that are
directed to our webserver.  
- When LibPCAP receives a packet, we store it in a hashmap with a key composed of 
`client IP + client PORT`.
- Then, when our webserver receives a request, we lookup the `client IP + client PORT` tuple 
 in the packets hashmap to find the right TCP SYN.

we are choosing that specific tuple for our lookup system because that's how the TCP stack 
uniquely identifies a connection in the kernel. To be more precise, it uses the tuple:
`(client_ip, client_port, server_ip, server_port)`.
There are useful writeups on 
[socket lookups in the kernel](https://thermalcircle.de/doku.php?id=blog:linux:sockets_in_the_linux_kernel_2_udp_socket_lookup_on_rx) that talk about this more in depth.

<Picture src="ebpf-fingerprint-webpcap2" height="340" />
<br />

Sadly, there is a problem with this architecture:
- libpcap is not real-time.
When the server thread will `accept()` a connection, it will try to lookup
packet data that hasn't been received yet.
- even when receiving low-latency data from libpcap, 
  copying and storing TCP packets into a hashmap is a slow operation.

This is where eBPF comes to the resque. What if I told you that with
eBPF we can generate that same hashmap of TCP SYN packets, but kernel side, without needing extra libraries and with little to no 
overhead on the system? The map will resize in kernel memory, but we'll maintain the ability to query 
it from userspace, by simply referring to a file descriptor.

Compared to the previous diagram, this new architecture simply replaces the 
libpcap thread with an eBPF program running kernel-side.

<Picture src="ebpf-fingerprint-webebpf2" height="380" />
<br />

Querying the map from userspace will simply require a function call,
something along the lines of `bpf_map_lookup_elem(map_reference, key)`.
All we'll need to do first is initialize a custom eBPF program.

### eBPF

The previous paragraph just provided a good introduction of what eBPF is,
and what can be done with it.

https://docs.cilium.io/en/stable/reference-guides/bpf/architecture/#maps

