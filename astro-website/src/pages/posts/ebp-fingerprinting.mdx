---
layout: '../../layouts/BlogPost.astro'
title: Implementing fast TCP fingerprinting with eBPF
subtitle: 
publishDate: 2025-06-01
description: 
tags: ['ebpf', 'tcp', 'fingerprinting']
permalink: https://halb.it/posts/ebpf-fingerprinting/
---
import Picture from '../../components/Picture.astro'
import Spoiler from '../../components/Spoiler.astro'

In this article I want to document my journey implementing fast TCP fingerprinting 
in a golang webserver, using eBPF. 
This system was developed as part of a larger anti-bot solution I've been
working on lately.<br/>

The background is that 
TCP fingerprinting is just one of the many signals that can be used to detect
unusual or identifying informations about a web request.
The rising need for human-generated data required to train large language models
has led to a huge increase in web scraping, accelerating the arms race
behind scraping and anti-scraping techonologies.



As a proof-of-concept, I implemented a standalone webserver that echoes back
a dump of the tc SYN/ACK of the client.

For the TCP side I started by implementing a simple proof-of-concept,
a standalone webserver that echoes back a dump of the
tcp SYN/ACK of the client. The source code is open-source on Github.

## http connections, from first principles

At its essence, an HTTP webserver is extremely simple.  
This should't be surprising: although browsers and the underlying protocols evolved and got [more complex](https://en.wikipedia.org/wiki/HTTP/3) over time, for 
compatibility reasons they 
still support the HTTP/1 protocol, which was [designed to be simple](https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/Evolution_of_HTTP).
An HTTP/1 web server can be implemented in just a few lines of C code.


As an example, I implemented a simple hello world webserver in C that we'll use as 
a starting point for the experiments in this article.
You don't really have to read the code for now, but keep in mind that its' here:

<Spoiler text="See code">

```c
#include <netinet/in.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>

#define PORT 8080

int main() {
  // Create socket file descriptor
  int server_fd = socket(AF_INET, SOCK_STREAM, 0);
  if (server_fd == 0) {
    perror("socket failed");
    exit(EXIT_FAILURE);
  }

  // Create a configuration struct with the network address and port
  struct sockaddr_in address = {
        .sin_family = AF_INET,
        .sin_addr.s_addr = INADDR_ANY,
        .sin_port = htons(PORT)
    };
  socklen_t address_length = sizeof(address);
  struct sockaddr *address_pointer = (struct sockaddr *)&address;

  // Bind the socket to the network address and port
  if (bind(server_fd, address_pointer, address_length) < 0) {
    perror("bind failed");
    exit(EXIT_FAILURE);
  }

  // Listen for connections
  if (listen(server_fd, 1) < 0) {
    perror("listen failed");
    exit(EXIT_FAILURE);
  }

  printf("Server listening on port %d...\n", PORT);

  while (1) {
    // Accept a new connection
    int new_socket = accept(server_fd, address_pointer, &address_length);
    if (new_socket < 0) {
      perror("accept failed");
      exit(EXIT_FAILURE);
    }

    // Send a response
    const char *response = "HTTP/1.0 200 OK\r\n"
                           "Content-Type: text/plain\r\n"
                           "Content-Length: 12\r\n"
                           "\r\n"
                           "Hello world ";

    send(new_socket, response, strlen(response), 0);

    // Close the connection
    close(new_socket);
  }

  return 0;
}
```

</Spoiler>

If we compile this code, run it, and visit `http://127.0.0.1:8080`.
from any modern browser, we'll be greeted by a hello world.

<Picture src="ebpf-fingerprint-browser1" height={490} alt="" />
<br/>

Beautiful, isn't it?

This code differs significantly from the hello worlds that you would normally write
with a modern web framework, regardles of what's trending today, for an important reason:  
Instead of using the abstractions that web frameworks provide to simplify the creation
of a web server,
we are directly using the abstraction layer provided by the operative system.

All the code in this example can be broken down into a few calls of the standard library.
Those functions are just thin wrappers around [syscalls](https://syscalls.mebeim.net/?table=x86/64/x64/latest) whith a similar name, which are all we need to implement a webserver on a POSIX system:

- we create a `socket`
- we `bind` it to a ip and port, and start to `listen`
- in an infinite loop, we call `accept()`, which blocks the execution
  until a client connects to our server
- after a connection is made, accept returns a file descriptor
  which we can `read` and `write` to like a simple file. At the other end of the
  line, the client will communicate with us in a similar way.

It's easy to see that beyond defining an ip and port we don't 
have to write any networking-related code, the OS is taking care of that for us.  
Because of that, if we want to understand what's happening at a deeper level we need to
capture and inspect the network traffic that reaches our webserver, which is something that
we can easily do with wireshark.

<Picture src="ebpf-fingerprint-wireshark1_crop" alt="" />
<br/>

To capture the packets shown in this screenshot I launched wireshark on the loopback interface, then I visited
the hello world website from the browser in order to generate some data.

With this webserver running and with our example traffic capture at hand, we can now
start to experiment with tcp traffic.

### Anatomy of a TCP handshake

Without going too much into details, TCP connections start with a famous three-way handhshake,
During which both the client and the server exchange some informations that 
are useful to establish a reliable connection.  

```
Client                      Server
  |                           |
  | --------- SYN ----------> |  Step 1: Client sends SYN
  |                           |
  | <----- SYN + ACK -------- |  Step 2: Server responds with SYN + ACK
  |                           |
  | --------- ACK ----------> |  Step 3: Client sends ACK
  |                           |
```

Historically, the informations exchanged during the handshake have always provided useful insight into 
the identity of the client. 

Let's have a look at the first TCP SYN in my packet captures:

<Picture src="ebpf-fingerprint-syn-color" alt="" />
<br/>

Broadly speaking, there are two kinds of informations we can gather from this data:
- informations about the Device that generated the packet
- informations about the route the packet travelled trough

A good overview of all these informations is provided by the authors
of the [ja4t TCP fingerprint](https://medium.com/foxio/ja4t-tcp-fingerprinting-12fb7ce9cb5a),
in an article I highly recommend. 
It's not my intention to repeat what's already been written extensively on the topic,
but in order to provide at least one example on the kind of data we gan gather from a packet,
i'll talk about one TCP option: MSS.

the Maximum Segment Size (MSS) TCP option can provide useful informations on the route of the packet.
In the packet we are analyzing, it's the value
highlighted in purple.  
This value represents the maximum size of TCP payload data that can be sent per packet,
and is dependent on the overhead in the network connection.  
For example, over ethernet we usually cannot send more than 1500 bytes over a single transaction,
so the MSS will be 1500 minus the size of the TCP and IP headers.

<Picture src="ebpf-fingerprint-tcp2z" height={100} alt="" />
<br/>

Observing an mss of 1460 would indicate a standard connection over ethernet.
a value smaller than that could indicate a vpn service. Or just a mobile network connection,
or GRE, or MSS clamping caused by [middleboxes](https://tma.roc.cnam.fr/Proceedings/TMA_Paper_22.pdf)...  
Network connections can be complicated, and we can't expect to learn 
everything from a single value. But at least it can provide some insight,
and some entropy for a fingerprinting system.

<Picture src="ebpf-fingerprint-tcp3z2" height={110} alt="" />
<br/>

Now, What about the MSS in the packet we are analyzing? 
If you look carefully, you'll see that the MSS is `65495`, 
way higher than what can fit in an Ethernet frame, and way off of 
any value we talked about so far.  
In a way, this is telling us something about the route too: 
`64Kb` is the [MTU of the loopback interface](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=0cf833aefaa85bbfce3ff70485e5534e09254773) on Linux.
This data is telling us that the packet never left the kernel, 
and lived in captivity without ever travelling the world.

### TCP fingerprinting: Making a plan

The goal of this project is to write a webserver that will echo back to the visitor 
its TCP fingerprinting data. For this to work, the webserver needs to directly 
access the TCP SYN data of the connections it receives.  
Is this something that we can do with the existing APIs we are using?

As it turns out, not really.
When our server accepts a new client connection, the `accept()` function returns
a new file descriptor.
We can use [getsockopt](https://man.archlinux.org/man/getsockopt.2.en) on that file descriptor
to receive some useful informations on the ongoing connection:
```c
getsockopt(sockfd, IPPROTO_TCP, TCP_INFO, &info, &len)
```

`IPPROTO_TCP` indicates that we are interested on the tcp-layer of the socket connection
`TCP_INFO` asks to receive information on the connection, causing the function to return a 
[tcp_info struct](https://github.com/torvalds/linux/blob/5b032cac622533631b8f9b7826498b7ce75001c6/include/uapi/linux/tcp.h#L229).

Sadly, if you have a quick look at the contents of that struct, you'll see that it only contains 
informations on the ongoing connection, nothing related to the original TCP SYN data.
There is a `tcpi_rcv_mss` value for example, but that's the result of a negotiation, 
not what the client originally sent.  
That's all we can get from the POSIX APIs.

### LibPCAP

The POSIX APIs don't provide easy access to TCP SYNs, but we can follow the same 
approach as wireshark, and just capture raw packets in a userspace buffer.
We can do this very easily with the LibPCAP library, which offers out of the box
a very simple system for filtering and capturing network packets.

A simple approach to implement our webserver could work like this:

- First, we start LibPCAP in a separate thread, capturing TCP SYN packets that are
directed to our webserver.  
- When LibPCAP receives a packet, we store it in a hashmap with a key composed of 
`client IP + client PORT`.
- Then, when our webserver receives a request, we lookup the `client IP + client PORT` tuple 
 in the packets hashmap to find the right TCP SYN.

we are choosing that specific tuple for our lookup system because that's how the TCP stack 
uniquely identifies a connection in the kernel. To be more precise, it uses the tuple:
`(client_ip, client_port, server_ip, server_port)`.
There are useful writeups on 
[socket lookups in the kernel](https://thermalcircle.de/doku.php?id=blog:linux:sockets_in_the_linux_kernel_2_udp_socket_lookup_on_rx) that talk about this more in depth.

<Picture src="ebpf-fingerprint-webpcap" height="300" />
<br />

Sadly, there is a problem with this architecture:
- libpcap is not real-time.
When the server thread will `accept()` a connection, it will try to lookup
packet data that hasn't been received yet.
- even when receiving low-latency data from libpcap, 
  copying and storing TCP packets into a hashmap is a slow operation.

This is where eBPF comes to the resque. What if I told you that with
eBPF we can generate that same hashmap of TCP SYN packets, but kernel side, with little to no 
overhead on the system? And we'll maintain the ability to query the map from userspace.

The same diagram would change into something like this:

<Picture src="ebpf-fingerprint-webebpf" height="350" />
<br />

Querying the map from userspace will be a simple function call,
something like
`bpf_map_lookup_elem(map_reference, key)`.

### eBPF


---




so it's safe to assume that the tuple client_ip, client_port that is returned by the accept() syscall when a client connects to our server uniquely identifies the conection.

After the accept(), our server can easily lookup the packets in the hashmap
populated by libpcap.

This is where we meet our first issue: 
-libpcap is not real-time.
when the server thread will accept() a connection, it will try to lookup
packet data that hasn't been received yet.
- even when receiving low-latency data from libpcap, 
  copying and storing a hashmap of tcp packets client-side is extremely slow.


Luckily, there is a different approach that can solve all our issues: eBPF.

We can write an eBPF program that filters all syn/ack packets sent to our server,
filtering for the server_ip+server_port tuple. the same program
can then store these packets into a hashmap, https://docs.cilium.io/en/stable/reference-guides/bpf/architecture/#maps which will reside in kernel memory, and which we can
query from user space by simply referring a file descriptor.
Pure magic!



